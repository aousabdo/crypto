---
title: "Analyzing Cryptocurrency Markets using R - Part 3 Rolling Correlations and More!"
author: "Dr. Aous Abdo @aousabdo"
output:
  html_document:
    df_print: paged
  html_notebook: default
editor_options:
  chunk_output_type: inline
---

In my two previous posts, [Downloading and Processing Crypto Data with R](http://rpubs.com/aousabdo/crypto_1) and [Corrleations in the Crypto World Part 1](http://rpubs.com/aousabdo/crypto_2) I went over how one can download crypto data in R and talked about static correlations for some coins. In this post, we will cover rolling correlatoins, a type of correlations we use with time-series data like crypto data. 

## R Libraries 
In this post we will be using the same libraries from my previous [post](http://rpubs.com/aousabdo/crypto_2). We will also reuse some of the functions introducec in that post. 

```{r message=FALSE, echo=FALSE}
library(PoloniexR)
library(data.table)
library(lubridate)
library(Quandl)
library(plyr)
library(stringr)
library(ggplot2)
library(plotly)
library(janitor)
library(quantmod)
library(pryr)
library(corrplot)
library(PerformanceAnalytics)
library(tidyr)
library(MLmetrics)
library(tidyquant)
library(corrr)
library(cowplot)
library(knitr)
library(zoo)
library(TTR)
library(kableExtra)
```


```{r poloniex_function, echo=FALSE}
get_alt_data <- function(tz = "UTC"
                         , coin = c("ETH", "LTC")
                         , add_bitcoin = TRUE
                         , return_in_USDT = TRUE
                         , from = "2017-01-01"
                         , to = "2018-04-09"
                         , period = "D"
                         , verbose = FALSE){
  
  # We will be using the public API
  poloniex.public <- PoloniexPublicAPI()
  
  # set the time zone to utc
  Sys.setenv(tz = tz)
  
  # convert from and to into time obj
  from  <- as.POSIXct(paste(from, tz, sep = ""))
  to    <- as.POSIXct(paste(to, tz, sep = ""))
  
  # lists to store data.tables and xts objects
  chart_list <- list()
  dt_list    <- list()
  
  # make sure the coin pair is in upper case
  coin       <- toupper(coin)
  coin_pairs <- paste0("BTC_", coin[coin != "BTC"])
  if(add_bitcoin | return_in_USDT) coin_pairs <- c("USDT_BTC", coin_pairs)
  
  # loop over the coins to get the data
  for(i in coin_pairs){
    if(verbose)
      invisible(cat('\tGetting data for ', i, ' pair\n'))
    
    # this is a list that will contain the chart data for each coin pair
    try(chart_list[[i]] <- ReturnChartData(theObject = poloniex.public
                                       , pair      = i
                                       , from      = from
                                       , to        = to
                                       , period    = period)
        , silent = TRUE)
    
    # list to contain data.tables 
    try(dt_list[[i]] <- as.data.table(chart_list[[i]]), silent = TRUE)
  }
  
  # convert to data.table and make sure to add a column containing the pairs
  coin_dt <- rbindlist(l = dt_list, use.names = TRUE, idcol = "pair")
  
  # return data in usdt prices
  if(return_in_USDT){
    # to get the price of the alt coin in usdt is not that simple but we'll do it
    # get a DT of the btc_usdt pair
    btc_usd <- coin_dt[pair == "USDT_BTC"]
    btc_usd <- btc_usd[, .(index, pair, weightedaverage)]
    setnames(btc_usd, c("Date", "USDT_BTC_pair", "USDT_BTC_price"))
    
    # get DT with only alt coins
    alt_coins <- copy(coin_dt)#[pair != "USDT_BTC"]
    
    # now we need to add an index to the alt_coins table, but first we have to rename the index column
    alt_coins[, Date := index]
    alt_coins[, index := 1:.N]
    setkey(alt_coins, index)
    
    # now merge the data tables
    coin_dt_usdt <- merge(x = alt_coins, y = btc_usd, by = "Date")
    
    # now calcualte the price in usdt
    coin_dt_usdt[, price_usdt := ifelse(pair == "USDT_BTC", USDT_BTC_price, weightedaverage * USDT_BTC_price)]
    
    # now get rid of the extra columns
    coin_dt_usdt[, c("USDT_BTC_price", "USDT_BTC_pair") := NULL]
    
    # we need to change some column names
    col_names_to_change <- c("pair", "high", "low", "open", "close", "volume", "quotevolume", "weightedaverage")
    col_names <- names(coin_dt_usdt)
    col_names[col_names %in% col_names_to_change] <- paste0(col_names_to_change, '_btc')
    
    setnames(coin_dt_usdt, col_names)
    
    # add a column for the usdt pair
    coin_dt_usdt[, pair_usdt := gsub("BTC_", "USDT_", pair_btc)]
    
    # adjust col order
    setcolorder(coin_dt_usdt, c(1:10, 12, 11))
    
    # set key again
    setkey(coin_dt_usdt, index)
    
    # now get rid of the index column since it is not needed anymore
    coin_dt_usdt[, index := NULL]
    
    # now put together the return list  
    return_list <- list(alt_chart_list = chart_list, alt_dt = coin_dt, alt_usdt_dt = coin_dt_usdt)
  }else{
    return_list <- list(alt_chart_list = chart_list, alt_dt = coin_dt)
  }
  
  return(return_list)
}
```


```{r, echo=FALSE}
# get alt data for some coins
alt_data <- get_alt_data(return_in_USDT = T
                         , from = "2015-01-01"
                         , coin = c('ETH','XRP', 'BCH', 'LTC', 'NEO', 'XMR', 'DASH', 'XEM'))[['alt_usdt_dt']]
# add daily price change
alt_data[, pct_change := Delt(price_usdt), by = pair_usdt]
```


## Rolling Correlations

Establishing a relationship, in this case a correlation, between two time series can be very benificial for several reasons, the most important of which is being able to use this relationship to predict market movement, which if you can acheive then you'll be rich. Established correlations between the price of a crypto currency, for example, with other time series data such as market sentiment, news, or the price of another crypto currency, can be used to better forecast the movement of that coin. But as we have seen in the previous [post](http://rpubs.com/aousabdo/crypto_2), one problem exists, these correlations are not static and they could vary greatly overtime.

Rolling correlations are similar to [moving averages](https://www.investopedia.com/university/movingaverage/movingaverages1.asp), they are metrics we apply over a time window to better see potential trends and avoid the noise in the data. Since this might be a new topic for some readers let's elaborate more. 

Suppose we have a dataset which reflects daily sales for a department store for three products, A, B, and C. The sales for these products are shown in the table below. 

![Department store sales](../figures/sales.PNG){width=500px}

One can see from the table that sales for a given product vary over time, with the greatest sales taking place over the weekends, Jan 20-21 and 27-28 in the sample shown. Let's calculate the moving average for product C over a period of 4 days for example. To calculate the moving average, or any rolling function, we first have to select a time window, in this case we selected 4 days. After that we apply the function, in this case calculating the average over the selected time window. Once that is done we shift the window by one unit, one day in this case, and reapply the function and so on. The figure below shows the moving averages for Product C over a period of 4 days.

![Department store sales moving average](../figures/sales2.PNG){width=500px}

One major factor in a rolling function is the time window over which this function is applied. Intuitively, one would expect the variability in the value returned to be lower for larger time windows. We show that in the figure below where we calcualte the moving averages for different time windows.
```{r}
# sales for product C
prod_c <- c(1030, 1195, 1228, 1178, 1504, 1697,1710 ,1491 ,1240 , 861, 853, 1158, 1265, 1566,1182)

# calculate 3 moving averages for time windows of 2, 4, and 7 days
sma2 <- rollmean(prod_c, k = 2)
sma4 <- rollmean(prod_c, k = 4)
sma7 <- rollmean(prod_c, k = 7)

# plot sales and overlay the moving averages
plot(prod_c, pch = 16, col = "black", xlab = "Index", ylab = "Sales", cex = 1.5, ylim = c(800, 1800))
points(sma2, col = "red", pch=16, cex = 1.5)
points(sma4, col = "blue", pch=16, cex = 1.5)
points(sma7, col = "green", pch=16, cex = 1.5)

# add legend
legend(0.8, 1820, legend=c("Sales", "2-day moving avg.", "4-day moving avg.", "7-day moving avg."),
       col=c("black", "red", "blue", "green"),  pch = rep(16, 4), cex=0.8)
```

Notice how increasing the time window helps us get rid of the noise in the data. This is evident from the smoother curves for the moving averages with time windows of 4 and 7. It is also worth noting that increasing the time window decreases the number number of retunred moving averages. This implies that a moving average over the whole period will result in just one number, which is simply the average over the whole period. 

The same can be done to any function that calculates a static value, like calculating correlations between two time series. 
Let's now calculate the rolling correlations over a period of 4 days for the sales of Products A and C. 

```{r}
# let's put the two sales of products A and C in a data.frame
prod_a <- c(250, 241, 251, 286, 365, 412, 415, 362, 301, 280, 250, 234, 215, 198, 300)
prod_c <- c(1030, 1195, 1228, 1178, 1504, 1697,1710 ,1491 ,1240 , 861, 853, 1158, 1265, 1566,1182)

df <- data.frame(prod_a, prod_c)

# now we'll calculate the rolling correlation
rollcorr_a_c <- runCor(df[, 1], df[, 2], n = 4)

# now let's make a plot
plot(rollcorr_a_c, pch = 19, col = "blue", xlab = "Index", ylab = "Correlation", cex = 1.5, type = "b")
abline(h = cor(df[, 1], df[2]), col = "red", lwd = 3)
legend(0.8, -0.5, legend=c("4-day Rolling Correlation", "Static Correlation"),
       col=c("blue", "red"),  lty=c(1, 1),pch=c(19, NA), cex=0.8)
```

As one can see from the above figure, for time series the correlation is not static and changes over time. In the figure the rolling correlation, shown in blue, varies greatly overtime, while the static correlation (shown in red) is constant.

### Rolling Correlation for Cryptocurrencies
We will be using the dataset we downloaded in our previous [post](http://rpubs.com/aousabdo/crypto_2). This dataset is shown in the table below. 

```{r alt_dataset}
kable(head(alt_data), "latex", booktabs=T) %>% column_spec(1, bold = T) %>% kable_as_image()
```

We will be performing our analysis on the daily returns, so we only need a subset of this dataset:
```{r}
alt_data_sub <- as.tibble(alt_data)
alt_data_sub %>% 
  select(Date, pair_btc, pct_change) %>% head() %>%
kable("latex", booktabs=T) %>% column_spec(1, bold = T) %>% kable_as_image()
```